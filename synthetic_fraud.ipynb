#Import Libraries and Functions

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    r2_score,
    root_mean_squared_error,
)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from typing import Tuple, Union


# Helper functions
def freedman_diaconis_bins(data: np.ndarray) -> int:
    """Calculate the optimal number of bins for a histogram using the Freedman-Diaconis rule.

    The Freedman-Diaconis rule is robust to outliers and works well for skewed data.
    It calculates the bin width using the interquartile range (IQR) and adjusts the 
    number of bins accordingly.

    Args:
        data (array-like): Input data, typically a 1D array of numerical values.

    Returns:
        int: Optimal number of bins for the histogram. If the bin width is very small, 
        a minimum of 1 bin is enforced.

    Example:
        >>> data = np.random.randn(1000)
        >>> freedman_diaconis_bins(data)
        16
    """
    # Ensure input is a NumPy array
    data = np.asarray(data)

    # Calculate the interquartile range (IQR)
    iqr = np.subtract(*np.percentile(data, [75, 25]))

    # Compute the bin width using the Freedman-Diaconis rule
    bin_width = 2 * iqr / np.cbrt(len(data))

    # If bin_width is too small, return at least 1 bin
    if bin_width <= 0:
        return 1

    # Calculate the number of bins and return it
    return max(1, int(np.ceil((data.max() - data.min()) / bin_width)))


def optimal_figsize(data: np.ndarray, bins: int = None, base_width: float = 8.0, base_height: float = 5.0) -> Tuple[float, float]:
    """Determine the optimal figure size based on the number of bins.

    Dynamically adjusts the width of the plot based on the number of bins (if 
    specified) or calculates the optimal number using the Freedman-Diaconis rule.
    The height is kept fixed or adjusted based on user input. Recommended for
    small datasets.

    Args:
        data (np.ndarray): The input data array to be visualized.
        bins (int, optional): The number of bins for the histogram. If not provided, 
            it is calculated using the Freedman-Diaconis rule.
        base_width (float, optional): The base width of the figure. Defaults to 8.0.
        base_height (float, optional): The base height of the figure. Defaults to 5.0.

    Returns:
        Tuple[float, float]: The optimal width and height of the figure.
    """
    # Calculate bins if not provided
    if bins is None:
        bins = freedman_diaconis_bins(data)
    
    # Adjust width based on the number of bins
    width = base_width + bins * 0.1
    return (width, base_height)


def optimal_figsize_by_range(data: np.ndarray, base_width: float = 8.0, base_height: float = 5.0, scale_factor: float = 0.5) -> Tuple[float, float]:
    """Determine the optimal figure size based on the data range.

    Adjusts the width of the figure proportionally to the range of the data to 
    ensure adequate spacing and visibility. The height remains fixed unless 
    adjusted by user input. Recommended for small datasets

    Args:
        data (np.ndarray): The input data array to be visualized.
        base_width (float, optional): The base width of the figure. Defaults to 8.0.
        base_height (float, optional): The base height of the figure. Defaults to 5.0.
        scale_factor (float, optional): Scaling factor to adjust the width based 
            on the data range. Defaults to 0.5.

    Returns:
        Tuple[float, float]: The optimal width and height of the figure.
    """
    # Calculate the range of the data
    data_range = np.max(data) - np.min(data)
    
    # Adjust width based on data range
    width = base_width + data_range * scale_factor
    return (width, base_height)


def figsize_by_samples(n_samples: int, base_width: float = 8.0, base_height: float = 5.0) -> Tuple[float, float]:
    """Determine the optimal figure size based on the number of samples.

    Adjusts the width of the figure proportionally to the number of samples, 
    ensuring adequate space for larger datasets. The height remains fixed 
    unless specified. Recommended for large datasets.

    Args:
        n_samples (int): The number of samples to be visualized.
        base_width (float, optional): The base width of the figure. Defaults to 8.0.
        base_height (float, optional): The base height of the figure. Defaults to 5.0.

    Returns:
        Tuple[float, float]: The optimal width and height of the figure.
    """
    # Adjust width based on the number of samples
    width = base_width + (n_samples // 1000)
    return (width, base_height)

#Import Data

from google.colab import files

uploaded = files.upload()

main_data = pd.read_csv("synthetic_fraud_dataset.csv")

#Data Exploration

# Data shape
main_data.shape

# First 10 rows of data
main_data.head(10)

# Data types
main_data.dtypes

##Q: How many unique `User_ID`, `Location`, `Merchant_Category` and `Card_Type` are present in the dataset?

unique_user_ID = main_data['User_ID'].nunique()
print("Number of unique User IDs:", unique_user_ID)
unique_location = main_data['Location'].nunique()
print("Number of unique Locations:", unique_location)
unique_merchant_category = main_data['Merchant_Category'].nunique()
print("Number of unique Merchant Categorys:", unique_merchant_category)
unique_card_type = main_data['Card_Type'].nunique()
print("Number of unique Card Types:", unique_card_type)

##Q: How many fraud transactions are present in the dataset?

fraud_transaction = main_data['Fraud_Label'].value_counts()
print("Number of fraud transactions:", fraud_transaction[1])

Q: What are the top five merchant categories and locations of the fraudulent transactions?

common_merchant_category = main_data[main_data["Fraud_Label"]==1]['Merchant_Category'].value_counts().head(5)
common_location = main_data[main_data["Fraud_Label"]==1]['Location'].value_counts().head(5)
print(f"Among all the fraudulent transactions the five most common merchant categories are: {common_merchant_category}")
print(f"Among all the fraudulent transactions the five most common locations are: {common_location}")
